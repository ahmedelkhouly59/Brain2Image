{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af5903ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import random\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57fb3250-1bc8-46f9-bd11-4dcfeff53d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "from keras.layers import Input, Layer, Dense, Conv2D, Conv2DTranspose\n",
    "from keras.layers import Flatten, Reshape, MaxPooling2D, UpSampling2D\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.layers import Activation, LeakyReLU\n",
    "\n",
    "from keras import regularizers, initializers\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import RandomUniform, Constant\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83e6215a-5c72-42cd-bbe1-5685a4afc0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcab4c2-b7fa-42e2-bfcc-8221747e3fd4",
   "metadata": {},
   "source": [
    "## EEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0884f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_eeg(path):\n",
    "    eeg = pickle.load(open(f'{path}/data.pkl', 'rb'), encoding='latin1')\n",
    "    x_train, x_test = eeg['x_train'], eeg['x_test']\n",
    "    y_train, y_test = eeg['y_train'], eeg['y_test']\n",
    "    x_train, x_vali, y_train, y_vali = train_test_split(x_train, y_train, test_size=0.20, random_state=42)\n",
    "    \n",
    "    eeg = {'x_train': x_train, 'y_train': y_train,\n",
    "                  'x_vali': x_vali, 'y_vali': y_vali,\n",
    "                  'x_test': x_test, 'y_test': y_test}\n",
    "\n",
    "    return eeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5297a7bb-0654-4c96-9871-438b9d79d147",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_eeg = load_eeg('datasets/thoughtviz/data/eeg/image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "265ec683-dab2-4da4-83b3-2e83f80c976e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36312, 14, 32, 1), (36312, 10))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_eeg['x_train'].shape, obj_eeg['y_train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7be9dd0a-0542-4ce3-9aec-ba4a0bb71bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThoughtViz:\n",
    "  def __init__(self):\n",
    "    channels, observations, num_classes = 14, 32, 10\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=(channels, observations, 1)))\n",
    "    model.add(Conv2D(32, (1, 4), activation='relu'))\n",
    "    model.add(Conv2D(25, (channels, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D((1, 3)))\n",
    "    model.add(Conv2D(50, (4, 25), activation='relu', data_format='channels_first'))\n",
    "    model.add(MaxPooling2D((1, 3)))\n",
    "    model.add(Conv2D(100, (50, 2), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model._name = 'classifier_encoder_thoughtviz'\n",
    "    self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5da4b406-f42f-43fc-96b8-5a48d39831c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"classifier_encoder_thoughtviz\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 14, 32, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 14, 29, 32)        160       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 1, 29, 25)         11225     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 1, 9, 25)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 50, 6, 1)          5050      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 50, 2, 1)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 1, 1, 100)         10100     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 100)               0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,449\n",
      "Trainable params: 38,047\n",
      "Non-trainable params: 402\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "eeg_classifier = ThoughtViz().model\n",
    "eeg_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4727e227-f93b-4a5c-a3c6-289bf5f94799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1135/1135 [==============================] - 31s 13ms/step - loss: 1.9764 - accuracy: 0.2998 - val_loss: 1.8288 - val_accuracy: 0.3381\n",
      "Epoch 2/30\n",
      "1135/1135 [==============================] - 12s 10ms/step - loss: 1.5860 - accuracy: 0.4683 - val_loss: 1.7827 - val_accuracy: 0.3809\n",
      "Epoch 3/30\n",
      "1135/1135 [==============================] - 13s 12ms/step - loss: 1.3177 - accuracy: 0.5716 - val_loss: 1.5866 - val_accuracy: 0.4594\n",
      "Epoch 4/30\n",
      "1135/1135 [==============================] - 13s 11ms/step - loss: 1.1049 - accuracy: 0.6531 - val_loss: 1.1299 - val_accuracy: 0.6344\n",
      "Epoch 5/30\n",
      "1135/1135 [==============================] - 13s 12ms/step - loss: 0.9339 - accuracy: 0.7138 - val_loss: 0.8935 - val_accuracy: 0.7300\n",
      "Epoch 6/30\n",
      "1135/1135 [==============================] - 12s 11ms/step - loss: 0.7883 - accuracy: 0.7619 - val_loss: 0.9025 - val_accuracy: 0.7160\n",
      "Epoch 7/30\n",
      "1135/1135 [==============================] - 13s 12ms/step - loss: 0.6781 - accuracy: 0.7968 - val_loss: 0.7641 - val_accuracy: 0.7634\n",
      "Epoch 8/30\n",
      "1135/1135 [==============================] - 13s 11ms/step - loss: 0.5967 - accuracy: 0.8256 - val_loss: 0.5766 - val_accuracy: 0.8351\n",
      "Epoch 9/30\n",
      "1135/1135 [==============================] - 13s 12ms/step - loss: 0.5226 - accuracy: 0.8505 - val_loss: 0.5366 - val_accuracy: 0.8392\n",
      "Epoch 10/30\n",
      "1135/1135 [==============================] - 13s 11ms/step - loss: 0.4808 - accuracy: 0.8640 - val_loss: 0.4326 - val_accuracy: 0.8793\n",
      "Epoch 11/30\n",
      "1135/1135 [==============================] - 12s 11ms/step - loss: 0.4360 - accuracy: 0.8760 - val_loss: 0.5534 - val_accuracy: 0.8318\n",
      "Epoch 12/30\n",
      "1135/1135 [==============================] - 13s 12ms/step - loss: 0.3956 - accuracy: 0.8871 - val_loss: 0.5913 - val_accuracy: 0.8044\n",
      "Epoch 13/30\n",
      "1135/1135 [==============================] - 13s 12ms/step - loss: 0.3640 - accuracy: 0.8978 - val_loss: 0.3973 - val_accuracy: 0.8803\n",
      "Epoch 14/30\n",
      "1135/1135 [==============================] - 14s 13ms/step - loss: 0.3328 - accuracy: 0.9076 - val_loss: 0.3972 - val_accuracy: 0.8852\n",
      "Epoch 15/30\n",
      "1135/1135 [==============================] - 13s 11ms/step - loss: 0.3101 - accuracy: 0.9147 - val_loss: 0.2983 - val_accuracy: 0.9187\n",
      "Epoch 16/30\n",
      "1135/1135 [==============================] - 14s 12ms/step - loss: 0.2886 - accuracy: 0.9185 - val_loss: 0.2587 - val_accuracy: 0.9353\n",
      "Epoch 17/30\n",
      "1135/1135 [==============================] - 13s 12ms/step - loss: 0.2782 - accuracy: 0.9225 - val_loss: 0.3129 - val_accuracy: 0.9101\n",
      "Epoch 18/30\n",
      "1135/1135 [==============================] - 13s 11ms/step - loss: 0.2529 - accuracy: 0.9298 - val_loss: 0.3437 - val_accuracy: 0.8952\n",
      "Epoch 19/30\n",
      "1135/1135 [==============================] - 13s 12ms/step - loss: 0.2373 - accuracy: 0.9342 - val_loss: 0.2171 - val_accuracy: 0.9473\n",
      "Epoch 20/30\n",
      "1135/1135 [==============================] - 12s 11ms/step - loss: 0.2265 - accuracy: 0.9386 - val_loss: 0.1854 - val_accuracy: 0.9577\n",
      "Epoch 21/30\n",
      "1135/1135 [==============================] - 12s 11ms/step - loss: 0.2130 - accuracy: 0.9432 - val_loss: 0.2444 - val_accuracy: 0.9316\n",
      "Epoch 22/30\n",
      "1135/1135 [==============================] - 11s 9ms/step - loss: 0.2042 - accuracy: 0.9431 - val_loss: 0.2724 - val_accuracy: 0.9219\n",
      "Epoch 23/30\n",
      "1135/1135 [==============================] - 12s 10ms/step - loss: 0.1918 - accuracy: 0.9475 - val_loss: 0.1956 - val_accuracy: 0.9509\n",
      "Epoch 24/30\n",
      "1135/1135 [==============================] - 10s 9ms/step - loss: 0.1805 - accuracy: 0.9508 - val_loss: 0.1700 - val_accuracy: 0.9594\n",
      "Epoch 25/30\n",
      "1135/1135 [==============================] - 11s 9ms/step - loss: 0.1719 - accuracy: 0.9543 - val_loss: 0.1576 - val_accuracy: 0.9614\n",
      "Epoch 26/30\n",
      "1135/1135 [==============================] - 10s 8ms/step - loss: 0.1675 - accuracy: 0.9539 - val_loss: 0.2045 - val_accuracy: 0.9426\n",
      "Epoch 27/30\n",
      "1135/1135 [==============================] - 11s 9ms/step - loss: 0.1630 - accuracy: 0.9568 - val_loss: 0.1959 - val_accuracy: 0.9473\n",
      "Epoch 28/30\n",
      "1135/1135 [==============================] - 11s 10ms/step - loss: 0.1504 - accuracy: 0.9581 - val_loss: 0.1507 - val_accuracy: 0.9608\n",
      "Epoch 29/30\n",
      "1135/1135 [==============================] - 9s 8ms/step - loss: 0.1412 - accuracy: 0.9628 - val_loss: 0.1758 - val_accuracy: 0.9566\n",
      "Epoch 30/30\n",
      "1135/1135 [==============================] - 9s 8ms/step - loss: 0.1421 - accuracy: 0.9621 - val_loss: 0.1374 - val_accuracy: 0.9646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x218b8e21bb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_classifier.compile(optimizer=keras.optimizers.legacy.SGD(learning_rate=1 * (10 ** -4), momentum=0.9, decay=1 * (10 ** -6)), \n",
    "                       loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.005, patience=50, restore_best_weights=True, verbose=1)\n",
    "callbacks = [early_stopping_callback]\n",
    "\n",
    "eeg_classifier.fit(obj_eeg['x_train'], obj_eeg['y_train'], epochs=30, validation_data=(obj_eeg['x_vali'], obj_eeg['y_vali']), batch_size=32, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3b27cfd-a489-44f6-be78-1cfc2e6d1625",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179/179 [==============================] - 1s 8ms/step - loss: 1.6777 - accuracy: 0.6725\n"
     ]
    }
   ],
   "source": [
    "eeg_classifier.save('models/ThoughtViz_obj_eeg.keras')\n",
    "loss, accuracy = eeg_classifier.evaluate(obj_eeg['x_test'], obj_eeg['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79c04407-471a-40f2-9dbc-6e37c5d8cf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.672450065612793"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
